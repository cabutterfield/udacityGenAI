{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LORA\n",
    "* Model: GPT2\n",
    "* Evaluation approach: AutoModelForSequenceClassification \n",
    "* Fine-tuning dataset: \"yelp_review_full\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df3217c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efe584a1c774c9fbd9603e232ca0033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 299M/299M [00:05<00:00, 59.1MB/s] \n",
      "Downloading data: 100%|██████████| 23.5M/23.5M [00:00<00:00, 55.8MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8008a37f2c348ce9fee49ae5c165114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd862de192142da8d681b4d883f712c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 520000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sms_spam dataset\n",
    "# See: https://huggingface.co/datasets/sms_spam\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=19\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170eabe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[426,\n",
       " 1244,\n",
       " 341,\n",
       " 312,\n",
       " 453,\n",
       " 359,\n",
       " 401,\n",
       " 679,\n",
       " 331,\n",
       " 407,\n",
       " 1532,\n",
       " 3850,\n",
       " 1413,\n",
       " 986,\n",
       " 1825,\n",
       " 448,\n",
       " 861,\n",
       " 433,\n",
       " 2544,\n",
       " 2287,\n",
       " 466,\n",
       " 89,\n",
       " 75,\n",
       " 900,\n",
       " 1707,\n",
       " 255,\n",
       " 768,\n",
       " 294,\n",
       " 1010,\n",
       " 540,\n",
       " 1071,\n",
       " 587,\n",
       " 1179,\n",
       " 1031,\n",
       " 384,\n",
       " 1053,\n",
       " 632,\n",
       " 724,\n",
       " 247,\n",
       " 242,\n",
       " 438,\n",
       " 617,\n",
       " 872,\n",
       " 2349,\n",
       " 508,\n",
       " 1005,\n",
       " 1086,\n",
       " 1140,\n",
       " 159,\n",
       " 3053,\n",
       " 621,\n",
       " 624,\n",
       " 103,\n",
       " 1350,\n",
       " 469,\n",
       " 664,\n",
       " 1662,\n",
       " 761,\n",
       " 986,\n",
       " 1556,\n",
       " 476,\n",
       " 374,\n",
       " 132,\n",
       " 1023,\n",
       " 520,\n",
       " 1799,\n",
       " 758,\n",
       " 455,\n",
       " 2144,\n",
       " 3369,\n",
       " 477,\n",
       " 1186,\n",
       " 707,\n",
       " 1646,\n",
       " 624,\n",
       " 826,\n",
       " 994,\n",
       " 1906,\n",
       " 762,\n",
       " 638,\n",
       " 541,\n",
       " 1562,\n",
       " 121,\n",
       " 553,\n",
       " 316,\n",
       " 1124,\n",
       " 308,\n",
       " 1216,\n",
       " 652,\n",
       " 853,\n",
       " 512,\n",
       " 1185,\n",
       " 115,\n",
       " 351,\n",
       " 271,\n",
       " 1257,\n",
       " 792,\n",
       " 594,\n",
       " 1431,\n",
       " 586,\n",
       " 4196,\n",
       " 734,\n",
       " 360,\n",
       " 1376,\n",
       " 164,\n",
       " 304,\n",
       " 696,\n",
       " 876,\n",
       " 641,\n",
       " 1101,\n",
       " 670,\n",
       " 580,\n",
       " 921,\n",
       " 1211,\n",
       " 210,\n",
       " 178,\n",
       " 1806,\n",
       " 364,\n",
       " 396,\n",
       " 978,\n",
       " 1366,\n",
       " 236,\n",
       " 452,\n",
       " 676,\n",
       " 732,\n",
       " 454,\n",
       " 822,\n",
       " 364,\n",
       " 783,\n",
       " 674,\n",
       " 1136,\n",
       " 1128,\n",
       " 439,\n",
       " 1231,\n",
       " 1692,\n",
       " 82,\n",
       " 97,\n",
       " 3027,\n",
       " 543,\n",
       " 769,\n",
       " 161,\n",
       " 2599,\n",
       " 348,\n",
       " 126,\n",
       " 847,\n",
       " 368,\n",
       " 748,\n",
       " 1078,\n",
       " 825,\n",
       " 1458,\n",
       " 868,\n",
       " 1960,\n",
       " 326,\n",
       " 1788,\n",
       " 266,\n",
       " 92,\n",
       " 1917,\n",
       " 1075,\n",
       " 1056,\n",
       " 759,\n",
       " 1973,\n",
       " 1129,\n",
       " 192,\n",
       " 733,\n",
       " 659,\n",
       " 188,\n",
       " 220,\n",
       " 370,\n",
       " 664,\n",
       " 834,\n",
       " 337,\n",
       " 301,\n",
       " 409,\n",
       " 233,\n",
       " 945,\n",
       " 824,\n",
       " 1155,\n",
       " 418,\n",
       " 1200,\n",
       " 756,\n",
       " 729,\n",
       " 537,\n",
       " 436,\n",
       " 490,\n",
       " 15,\n",
       " 503,\n",
       " 1389,\n",
       " 210,\n",
       " 1745,\n",
       " 236,\n",
       " 161,\n",
       " 2457,\n",
       " 721,\n",
       " 312,\n",
       " 492,\n",
       " 661,\n",
       " 99,\n",
       " 200,\n",
       " 1516,\n",
       " 394,\n",
       " 1233,\n",
       " 386,\n",
       " 581,\n",
       " 6,\n",
       " 1113,\n",
       " 1307,\n",
       " 1750,\n",
       " 525,\n",
       " 350,\n",
       " 2469,\n",
       " 951,\n",
       " 1628,\n",
       " 71,\n",
       " 177,\n",
       " 77,\n",
       " 1360,\n",
       " 912,\n",
       " 745,\n",
       " 230,\n",
       " 481,\n",
       " 583,\n",
       " 212,\n",
       " 404,\n",
       " 1001,\n",
       " 59,\n",
       " 2527,\n",
       " 77,\n",
       " 424,\n",
       " 1359,\n",
       " 374,\n",
       " 2624,\n",
       " 358,\n",
       " 923,\n",
       " 662,\n",
       " 873,\n",
       " 1129,\n",
       " 2063,\n",
       " 624,\n",
       " 154,\n",
       " 723,\n",
       " 473,\n",
       " 324,\n",
       " 293,\n",
       " 2996,\n",
       " 182,\n",
       " 559,\n",
       " 698,\n",
       " 615,\n",
       " 1451,\n",
       " 114,\n",
       " 133,\n",
       " 294,\n",
       " 667,\n",
       " 409,\n",
       " 1631,\n",
       " 148,\n",
       " 88,\n",
       " 206,\n",
       " 416,\n",
       " 695,\n",
       " 775,\n",
       " 690,\n",
       " 160,\n",
       " 129,\n",
       " 137,\n",
       " 271,\n",
       " 288,\n",
       " 356,\n",
       " 272,\n",
       " 242,\n",
       " 603,\n",
       " 911,\n",
       " 881,\n",
       " 420,\n",
       " 739,\n",
       " 379,\n",
       " 1731,\n",
       " 94,\n",
       " 243,\n",
       " 2253,\n",
       " 108,\n",
       " 377,\n",
       " 158,\n",
       " 286,\n",
       " 320,\n",
       " 1379,\n",
       " 123,\n",
       " 386,\n",
       " 3212,\n",
       " 366,\n",
       " 315,\n",
       " 593,\n",
       " 133,\n",
       " 452,\n",
       " 710,\n",
       " 2099,\n",
       " 526,\n",
       " 352,\n",
       " 962,\n",
       " 377,\n",
       " 1726,\n",
       " 76,\n",
       " 1076,\n",
       " 594,\n",
       " 1292,\n",
       " 529,\n",
       " 1819,\n",
       " 276,\n",
       " 651,\n",
       " 1197,\n",
       " 2393,\n",
       " 916,\n",
       " 681,\n",
       " 1702,\n",
       " 274,\n",
       " 377,\n",
       " 294,\n",
       " 797,\n",
       " 2089,\n",
       " 511,\n",
       " 872,\n",
       " 608,\n",
       " 3188,\n",
       " 82,\n",
       " 1259,\n",
       " 537,\n",
       " 1056,\n",
       " 631,\n",
       " 1416,\n",
       " 1094,\n",
       " 342,\n",
       " 90,\n",
       " 780,\n",
       " 817,\n",
       " 1803,\n",
       " 338,\n",
       " 376,\n",
       " 331,\n",
       " 551,\n",
       " 822,\n",
       " 4618,\n",
       " 384,\n",
       " 3988,\n",
       " 1247,\n",
       " 1107,\n",
       " 728,\n",
       " 569,\n",
       " 1504,\n",
       " 129,\n",
       " 39,\n",
       " 229,\n",
       " 521,\n",
       " 508,\n",
       " 346,\n",
       " 795,\n",
       " 1103,\n",
       " 1546,\n",
       " 306,\n",
       " 131,\n",
       " 506,\n",
       " 146,\n",
       " 1018,\n",
       " 529,\n",
       " 1551,\n",
       " 760,\n",
       " 398,\n",
       " 725,\n",
       " 667,\n",
       " 4267,\n",
       " 1699,\n",
       " 859,\n",
       " 2791,\n",
       " 284,\n",
       " 555,\n",
       " 409,\n",
       " 286,\n",
       " 278,\n",
       " 333,\n",
       " 894,\n",
       " 189,\n",
       " 879,\n",
       " 426,\n",
       " 92,\n",
       " 384,\n",
       " 140,\n",
       " 1193,\n",
       " 151,\n",
       " 469,\n",
       " 614,\n",
       " 310,\n",
       " 228,\n",
       " 380,\n",
       " 282,\n",
       " 1119,\n",
       " 476,\n",
       " 719,\n",
       " 2514,\n",
       " 298,\n",
       " 215,\n",
       " 317,\n",
       " 383,\n",
       " 495,\n",
       " 1257,\n",
       " 1559,\n",
       " 399,\n",
       " 324,\n",
       " 894,\n",
       " 367,\n",
       " 1083,\n",
       " 904,\n",
       " 391,\n",
       " 1661,\n",
       " 1097,\n",
       " 88,\n",
       " 673,\n",
       " 2062,\n",
       " 1198,\n",
       " 159,\n",
       " 591,\n",
       " 150,\n",
       " 763,\n",
       " 356,\n",
       " 219,\n",
       " 829,\n",
       " 493,\n",
       " 960,\n",
       " 939,\n",
       " 203,\n",
       " 434,\n",
       " 239,\n",
       " 463,\n",
       " 261,\n",
       " 1537,\n",
       " 591,\n",
       " 961,\n",
       " 549,\n",
       " 1144,\n",
       " 1347,\n",
       " 211,\n",
       " 481,\n",
       " 442,\n",
       " 350,\n",
       " 140,\n",
       " 1394,\n",
       " 226,\n",
       " 469,\n",
       " 500,\n",
       " 148,\n",
       " 1233,\n",
       " 890,\n",
       " 697,\n",
       " 988,\n",
       " 501,\n",
       " 651,\n",
       " 903,\n",
       " 727,\n",
       " 540,\n",
       " 166,\n",
       " 471,\n",
       " 513,\n",
       " 968,\n",
       " 1005,\n",
       " 2081,\n",
       " 397,\n",
       " 442,\n",
       " 515,\n",
       " 157,\n",
       " 791,\n",
       " 291,\n",
       " 202,\n",
       " 128,\n",
       " 601,\n",
       " 2401,\n",
       " 335,\n",
       " 1492,\n",
       " 192,\n",
       " 244,\n",
       " 318,\n",
       " 1273,\n",
       " 1083,\n",
       " 1680,\n",
       " 912,\n",
       " 201,\n",
       " 1502,\n",
       " 788,\n",
       " 3019,\n",
       " 932,\n",
       " 1328,\n",
       " 1039,\n",
       " 3,\n",
       " 859,\n",
       " 290,\n",
       " 144,\n",
       " 1585,\n",
       " 430,\n",
       " 602,\n",
       " 1660,\n",
       " 331,\n",
       " 489,\n",
       " 788,\n",
       " 576,\n",
       " 1600,\n",
       " 435,\n",
       " 1428,\n",
       " 108,\n",
       " 200,\n",
       " 579,\n",
       " 133,\n",
       " 411,\n",
       " 61,\n",
       " 1300,\n",
       " 699,\n",
       " 497,\n",
       " 322,\n",
       " 273,\n",
       " 413,\n",
       " 134,\n",
       " 1066,\n",
       " 19,\n",
       " 578,\n",
       " 150,\n",
       " 1722,\n",
       " 329,\n",
       " 974,\n",
       " 452,\n",
       " 629,\n",
       " 1395,\n",
       " 402,\n",
       " 770,\n",
       " 367,\n",
       " 539,\n",
       " 254,\n",
       " 684,\n",
       " 1090,\n",
       " 2043,\n",
       " 401,\n",
       " 816,\n",
       " 320,\n",
       " 183,\n",
       " 1322,\n",
       " 3762,\n",
       " 414,\n",
       " 89,\n",
       " 501,\n",
       " 406,\n",
       " 616,\n",
       " 115,\n",
       " 762,\n",
       " 601,\n",
       " 728,\n",
       " 220,\n",
       " 1067,\n",
       " 2968,\n",
       " 209,\n",
       " 277,\n",
       " 458,\n",
       " 600,\n",
       " 100,\n",
       " 1040,\n",
       " 580,\n",
       " 786,\n",
       " 564,\n",
       " 1153,\n",
       " 359,\n",
       " 159,\n",
       " 287,\n",
       " 1483,\n",
       " 391,\n",
       " 1229,\n",
       " 356,\n",
       " 638,\n",
       " 531,\n",
       " 125,\n",
       " 645,\n",
       " 166,\n",
       " 1671,\n",
       " 208,\n",
       " 223,\n",
       " 243,\n",
       " 1084,\n",
       " 770,\n",
       " 388,\n",
       " 500,\n",
       " 2678,\n",
       " 327,\n",
       " 69,\n",
       " 618,\n",
       " 348,\n",
       " 460,\n",
       " 792,\n",
       " 69,\n",
       " 118,\n",
       " 439,\n",
       " 2117,\n",
       " 174,\n",
       " 172,\n",
       " 352,\n",
       " 1084,\n",
       " 387,\n",
       " 287,\n",
       " 157,\n",
       " 436,\n",
       " 1992,\n",
       " 1435,\n",
       " 199,\n",
       " 950,\n",
       " 2213,\n",
       " 282,\n",
       " 1590,\n",
       " 448,\n",
       " 367,\n",
       " 494,\n",
       " 422,\n",
       " 332,\n",
       " 1225,\n",
       " 1664,\n",
       " 1124,\n",
       " 164,\n",
       " 109,\n",
       " 2254,\n",
       " 232,\n",
       " 441,\n",
       " 440,\n",
       " 2459,\n",
       " 828,\n",
       " 1174,\n",
       " 699,\n",
       " 502,\n",
       " 603,\n",
       " 508,\n",
       " 430,\n",
       " 1126,\n",
       " 210,\n",
       " 286,\n",
       " 336,\n",
       " 126,\n",
       " 119,\n",
       " 377,\n",
       " 317,\n",
       " 329,\n",
       " 137,\n",
       " 523,\n",
       " 840,\n",
       " 615,\n",
       " 583,\n",
       " 598,\n",
       " 132,\n",
       " 621,\n",
       " 398,\n",
       " 633,\n",
       " 1082,\n",
       " 532,\n",
       " 302,\n",
       " 299,\n",
       " 1018,\n",
       " 581,\n",
       " 911,\n",
       " 181,\n",
       " 136,\n",
       " 386,\n",
       " 224,\n",
       " 761,\n",
       " 2665,\n",
       " 163,\n",
       " 108,\n",
       " 500,\n",
       " 396,\n",
       " 358,\n",
       " 129,\n",
       " 225,\n",
       " 258,\n",
       " 655,\n",
       " 342,\n",
       " 491,\n",
       " 295,\n",
       " 409,\n",
       " 363,\n",
       " 707,\n",
       " 247,\n",
       " 311,\n",
       " 290,\n",
       " 527,\n",
       " 751,\n",
       " 846,\n",
       " 128,\n",
       " 560,\n",
       " 476,\n",
       " 1273,\n",
       " 114,\n",
       " 622,\n",
       " 473,\n",
       " 1955,\n",
       " 322,\n",
       " 1280,\n",
       " 1538,\n",
       " 1167,\n",
       " 522,\n",
       " 223,\n",
       " 503,\n",
       " 1465,\n",
       " 194,\n",
       " 522,\n",
       " 1463,\n",
       " 517,\n",
       " 409,\n",
       " 153,\n",
       " 166,\n",
       " 332,\n",
       " 558,\n",
       " 756,\n",
       " 487,\n",
       " 388,\n",
       " 698,\n",
       " 650,\n",
       " 169,\n",
       " 186,\n",
       " 1518,\n",
       " 580,\n",
       " 404,\n",
       " 708,\n",
       " 337,\n",
       " 2145,\n",
       " 1253,\n",
       " 788,\n",
       " 986,\n",
       " 484,\n",
       " 1095,\n",
       " 833,\n",
       " 1309,\n",
       " 1992,\n",
       " 329,\n",
       " 222,\n",
       " 2128,\n",
       " 2068,\n",
       " 566,\n",
       " 839,\n",
       " 390,\n",
       " 381,\n",
       " 899,\n",
       " 689,\n",
       " 306,\n",
       " 576,\n",
       " 446,\n",
       " 350,\n",
       " 319,\n",
       " 446,\n",
       " 88,\n",
       " 587,\n",
       " 788,\n",
       " 620,\n",
       " 234,\n",
       " 857,\n",
       " 1231,\n",
       " 457,\n",
       " 1380,\n",
       " 105,\n",
       " 1301,\n",
       " 1817,\n",
       " 883,\n",
       " 261,\n",
       " 140,\n",
       " 4231,\n",
       " 801,\n",
       " 606,\n",
       " 655,\n",
       " 1948,\n",
       " 144,\n",
       " 308,\n",
       " 837,\n",
       " 799,\n",
       " 967,\n",
       " 521,\n",
       " 265,\n",
       " 605,\n",
       " 1045,\n",
       " 196,\n",
       " 280,\n",
       " 22,\n",
       " 889,\n",
       " 206,\n",
       " 1373,\n",
       " 548,\n",
       " 744,\n",
       " 177,\n",
       " 612,\n",
       " 548,\n",
       " 546,\n",
       " 260,\n",
       " 1734,\n",
       " 1210,\n",
       " 1077,\n",
       " 138,\n",
       " 1366,\n",
       " 1425,\n",
       " 290,\n",
       " 2125,\n",
       " 582,\n",
       " 448,\n",
       " 324,\n",
       " 92,\n",
       " 78,\n",
       " 458,\n",
       " 27,\n",
       " 228,\n",
       " 1648,\n",
       " 494,\n",
       " 963,\n",
       " 1775,\n",
       " 1958,\n",
       " 225,\n",
       " 2047,\n",
       " 330,\n",
       " 1777,\n",
       " 357,\n",
       " 426,\n",
       " 106,\n",
       " 1201,\n",
       " 1078,\n",
       " 237,\n",
       " 535,\n",
       " 262,\n",
       " 152,\n",
       " 1008,\n",
       " 627,\n",
       " 78,\n",
       " 697,\n",
       " 1542,\n",
       " 149,\n",
       " 467,\n",
       " 101,\n",
       " 4056,\n",
       " 989,\n",
       " 723,\n",
       " 1075,\n",
       " 219,\n",
       " 199,\n",
       " 1144,\n",
       " 420,\n",
       " 645,\n",
       " 302,\n",
       " 1824,\n",
       " 183,\n",
       " 522,\n",
       " 250,\n",
       " 610,\n",
       " 913,\n",
       " 597,\n",
       " 602,\n",
       " 994,\n",
       " 365,\n",
       " 1419,\n",
       " 1417,\n",
       " 345,\n",
       " 98,\n",
       " 1252,\n",
       " 1868,\n",
       " 1767,\n",
       " 593,\n",
       " 26,\n",
       " 169,\n",
       " 346,\n",
       " 6,\n",
       " 252,\n",
       " 1124,\n",
       " 736,\n",
       " 481,\n",
       " 493,\n",
       " 271,\n",
       " 111,\n",
       " 2656,\n",
       " 843,\n",
       " 333,\n",
       " 60,\n",
       " 619,\n",
       " 254,\n",
       " 775,\n",
       " 153,\n",
       " 2013,\n",
       " 368,\n",
       " 231,\n",
       " 645,\n",
       " 233,\n",
       " 633,\n",
       " 255,\n",
       " 729,\n",
       " 171,\n",
       " 1235,\n",
       " 103,\n",
       " 300,\n",
       " 165,\n",
       " 380,\n",
       " 117,\n",
       " 811,\n",
       " 206,\n",
       " 466,\n",
       " 164,\n",
       " 140,\n",
       " 574,\n",
       " 1710,\n",
       " 369,\n",
       " 2695,\n",
       " 150,\n",
       " 1306,\n",
       " 228,\n",
       " 944,\n",
       " 982,\n",
       " 1276,\n",
       " 296,\n",
       " 585,\n",
       " 1363,\n",
       " 564,\n",
       " 2608,\n",
       " 862,\n",
       " 425,\n",
       " 379,\n",
       " 2981,\n",
       " 1560,\n",
       " 1072,\n",
       " 867,\n",
       " 2117,\n",
       " 75,\n",
       " 755,\n",
       " 1100,\n",
       " 759,\n",
       " 347,\n",
       " 1144,\n",
       " 141,\n",
       " 1339,\n",
       " 333,\n",
       " 884,\n",
       " 495,\n",
       " 883,\n",
       " 828,\n",
       " 303,\n",
       " 300,\n",
       " 1030,\n",
       " 725,\n",
       " 134,\n",
       " 312,\n",
       " 556,\n",
       " 356,\n",
       " 403,\n",
       " 132,\n",
       " 530,\n",
       " 1219,\n",
       " 1127,\n",
       " 575,\n",
       " 575,\n",
       " 69,\n",
       " 941,\n",
       " 369,\n",
       " 1966,\n",
       " 368,\n",
       " 377,\n",
       " 2011,\n",
       " 1595,\n",
       " 4084,\n",
       " 110,\n",
       " 299,\n",
       " 469,\n",
       " 632,\n",
       " 412,\n",
       " 1097,\n",
       " 314,\n",
       " 737,\n",
       " 325,\n",
       " 335,\n",
       " 583,\n",
       " 344,\n",
       " 101,\n",
       " 552,\n",
       " 612,\n",
       " 3364,\n",
       " 1306,\n",
       " 1034,\n",
       " 590,\n",
       " 429,\n",
       " 1882,\n",
       " 292,\n",
       " 717,\n",
       " 2533,\n",
       " 3073,\n",
       " 1228,\n",
       " 3008,\n",
       " 1332,\n",
       " 1384,\n",
       " 1922,\n",
       " 203,\n",
       " 695,\n",
       " 1157,\n",
       " 1407,\n",
       " 2565,\n",
       " 1480,\n",
       " 490,\n",
       " 412,\n",
       " 1051,\n",
       " 224,\n",
       " 392,\n",
       " 667,\n",
       " 444,\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in dataset['train']['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60fb1a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f9539a44734af3ae5ba8bf5682686d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c90bfe5262246c1821f8383079d1a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b85e75ab4946b89bf37d54d018f339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca509b8b0874e52ac20734c7fb789fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811bedea89594102a9362d2864c8e34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, max_length=100, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e352f281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc89cc14db240cf8ebb7905ff449e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/520000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d643be6c0d4e8aa3c5cfd01441b8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399fbb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 520000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 130000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667aa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train= tokenized_datasets['train'].shuffle(seed=100).select(range(10000))\n",
    "small_test= tokenized_datasets['test'].shuffle(seed=100).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3692c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4604ce3d954b4c0f928bc7d1281cc99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=5)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de381bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea5af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e434c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",per_device_train_batch_size = 1,\n",
    "        per_device_eval_batch_size = 1, num_train_epochs=1,save_strategy='epoch', logging_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24aaed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 499183 KiB | 499183 KiB | 499183 KiB |      0 B   |\n",
      "|       from large pool | 486400 KiB | 486400 KiB | 486400 KiB |      0 B   |\n",
      "|       from small pool |  12783 KiB |  12783 KiB |  12783 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 499183 KiB | 499183 KiB | 499183 KiB |      0 B   |\n",
      "|       from large pool | 486400 KiB | 486400 KiB | 486400 KiB |      0 B   |\n",
      "|       from small pool |  12783 KiB |  12783 KiB |  12783 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 498396 KiB | 498396 KiB | 498396 KiB |      0 B   |\n",
      "|       from large pool | 485619 KiB | 485619 KiB | 485619 KiB |      0 B   |\n",
      "|       from small pool |  12777 KiB |  12777 KiB |  12777 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 555008 KiB | 555008 KiB | 555008 KiB |      0 B   |\n",
      "|       from large pool | 540672 KiB | 540672 KiB | 540672 KiB |      0 B   |\n",
      "|       from small pool |  14336 KiB |  14336 KiB |  14336 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  55825 KiB |  55849 KiB | 269821 KiB | 213996 KiB |\n",
      "|       from large pool |  54272 KiB |  54272 KiB | 261632 KiB | 207360 KiB |\n",
      "|       from small pool |   1553 KiB |   2045 KiB |   8189 KiB |   6636 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     173    |     173    |     173    |       0    |\n",
      "|       from large pool |      50    |      50    |      50    |       0    |\n",
      "|       from small pool |     123    |     123    |     123    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     173    |     173    |     173    |       0    |\n",
      "|       from large pool |      50    |      50    |      50    |       0    |\n",
      "|       from small pool |     123    |     123    |     123    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      27    |       0    |\n",
      "|       from large pool |      20    |      20    |      20    |       0    |\n",
      "|       from small pool |       7    |       7    |       7    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      21    |      21    |      26    |       5    |\n",
      "|       from large pool |      19    |      19    |      19    |       0    |\n",
      "|       from small pool |       2    |       2    |       7    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee92c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c126342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.522686004638672,\n",
       " 'eval_accuracy': 0.208,\n",
       " 'eval_runtime': 17.8327,\n",
       " 'eval_samples_per_second': 56.077,\n",
       " 'eval_steps_per_second': 56.077}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e3126d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "config = LoraConfig(task_type=\"SEQ_CLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d71d69f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce892e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55ca7b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2ForSequenceClassification(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Linear(\n",
      "                in_features=768, out_features=2304, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (score): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb188f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 302,592 || all params: 124,742,400 || trainable%: 0.24257349545944282\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec552c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = small_train.rename_column('label','labels')\n",
    "small_test = small_test.rename_column('label','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "999ed085",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",per_device_train_batch_size = 1,\n",
    "        per_device_eval_batch_size = 1, num_train_epochs=1,save_strategy='epoch', save_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fc0ef64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 08:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.298100</td>\n",
       "      <td>1.243794</td>\n",
       "      <td>0.457000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer/checkpoint-10000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=1.6284845581054688, metrics={'train_runtime': 538.3117, 'train_samples_per_second': 18.577, 'train_steps_per_second': 18.577, 'total_flos': 512151552000000.0, 'train_loss': 1.6284845581054688, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4c4ebb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56875600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1398eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"gpt-lora-final\")\n",
    "#my_lora_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt-lora-part2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43716aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModelForSequenceClassification\n",
    "\n",
    "saved_model_directory=\"/workspace/gpt-lora-final\"\n",
    "\n",
    "#PeftModelForSequenceClassification.from_pretrained(saved_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "955a6185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForSequenceClassification(\n",
       "      (base_model): LoraModel(\n",
       "        (model): GPT2ForSequenceClassification(\n",
       "          (transformer): GPT2Model(\n",
       "            (wte): Embedding(50257, 768)\n",
       "            (wpe): Embedding(1024, 768)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (h): ModuleList(\n",
       "              (0-11): 12 x GPT2Block(\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): GPT2Attention(\n",
       "                  (c_attn): Linear(\n",
       "                    in_features=768, out_features=2304, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (c_proj): Conv1D()\n",
       "                  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): GPT2MLP(\n",
       "                  (c_fc): Conv1D()\n",
       "                  (c_proj): Conv1D()\n",
       "                  (act): NewGELUActivation()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (score): ModulesToSaveWrapper(\n",
       "            (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=5, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PeftModelForSequenceClassification.from_pretrained(lora_model,saved_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da3c111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model=PeftModelForSequenceClassification.from_pretrained(lora_model,'gpt-lora-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5301564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2be3ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.522686004638672,\n",
       " 'eval_accuracy': 0.208,\n",
       " 'eval_runtime': 21.1109,\n",
       " 'eval_samples_per_second': 47.369,\n",
       " 'eval_steps_per_second': 47.369}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
